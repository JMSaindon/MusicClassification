{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ce notebook contient les mêmes méthodes que le premier mais executées sur les features extraites par Mr. Hanna\n",
    "# contrairement à l'autre qui repose sur notre propre extraction avec librosa\n",
    "\n",
    "# Executez cette case si vous n'avez pas les données dans le répertoire\n",
    "\n",
    "# !wget http://dept-info.labri.fr/~hanna/Pub/features_adapte.csv\n",
    "# !wget http://dept-info.labri.fr/~hanna/Pub/features_head.csv\n",
    "# !wget http://dept-info.labri.fr/~hanna/Pub/train_clean.csv\n",
    "# !wget http://dept-info.labri.fr/~hanna/Pub/test_clean.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import random as rd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['feature', 'chroma_cens', 'chroma_cens.1', 'chroma_cens.2',\n",
      "       'chroma_cens.3', 'chroma_cens.4', 'chroma_cens.5', 'chroma_cens.6',\n",
      "       'chroma_cens.7', 'chroma_cens.8',\n",
      "       ...\n",
      "       'tonnetz.39', 'tonnetz.40', 'tonnetz.41', 'zcr', 'zcr.1', 'zcr.2',\n",
      "       'zcr.3', 'zcr.4', 'zcr.5', 'zcr.6'],\n",
      "      dtype='object', length=519)\n",
      "#################\n",
      "test data: (4002, 519)\n",
      "all test id: 4008\n"
     ]
    }
   ],
   "source": [
    "# Croisement features/tracks du dataset train\n",
    "\n",
    "# Nom des features\n",
    "features = pd.read_csv(filepath_or_buffer=\"features_head.csv\", sep=\",\")\n",
    "print(features.columns)\n",
    "#print(features)\n",
    "print(\"#################\")\n",
    "\n",
    "# Croisement features/tracks du dataset train\n",
    "traingenre = pd.read_csv(filepath_or_buffer=\"train_clean.csv\", sep=\",\")\n",
    "iter_csv = pd.read_csv(filepath_or_buffer=\"features_adapte.csv\", sep=\",\", iterator=True, chunksize=10000)\n",
    "datatrain = pd.concat([chunk for chunk in iter_csv])\n",
    "\n",
    "train_data = pd.merge(traingenre, datatrain, on='track_id')\n",
    "train_data.sample(n=10)\n",
    "\n",
    "testgenre = pd.read_csv(filepath_or_buffer=\"test_clean.csv\", sep=\",\")\n",
    "test_data = pd.merge(testgenre, datatrain, on='track_id')\n",
    "test_data.drop(['genre_id'], axis=1, inplace=True)\n",
    "print(f\"test data: {test_data.shape}\")\n",
    "\n",
    "all_test_id = pd.read_csv(filepath_or_buffer=\"submission.csv\", sep=\",\")\n",
    "all_test_id.drop(['genre_id'], axis=1, inplace=True)\n",
    "print(f\"all test id: {len(all_test_id)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (3995, 518), y_train: (3995,), x_test: (4002, 518)\n"
     ]
    }
   ],
   "source": [
    "# training sets\n",
    "x_train = train_data.drop(['genre_id', 'track_id'], axis=1)\n",
    "y_train = train_data['genre_id'].values\n",
    "x_test  = test_data.drop(['track_id'], axis=1)\n",
    "test_id = test_data['track_id'].values\n",
    "\n",
    "print(f\"x_train: {x_train.shape}, y_train: {y_train.shape}, x_test: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_train: -1.815250402489839e-16. std_train: 0.9961315135452709\n"
     ]
    }
   ],
   "source": [
    "# normalisation\n",
    "scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "mean_train = x_train.mean()\n",
    "std_train = x_train.std()\n",
    "\n",
    "print(f'mean_train: {mean_train}. std_train: {std_train}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.2, max_delta_step=0, max_depth=5,\n",
       "              min_child_weight=3, missing=None, n_estimators=180, n_jobs=1,\n",
       "              nthread=None, objective='multi:softprob', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgb.XGBClassifier(n_estimators=180, learning_rate=0.2, max_depth=5, min_child_weight=3)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "assert len(y_pred) == len(test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create kaggle csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted id: 4002, submitting 4008 ids\n"
     ]
    }
   ],
   "source": [
    "output_data = pd.DataFrame({'track_id': test_id, 'genre_id': y_pred})\n",
    "output_data = output_data.merge(all_test_id, on='track_id', how='right')\n",
    "\n",
    "# fill missing id with a random genre\n",
    "output_data['genre_id'] = output_data['genre_id'].apply(lambda x: rd.randint(1,8) if np.isnan(x) else x)\n",
    "\n",
    "output_data['genre_id'] = output_data['genre_id'].apply(int)\n",
    "output_data.set_index('track_id', inplace=True)\n",
    "print(f\"predicted id: {len(test_id)}, submitting {len(output_data)} ids\")\n",
    "\n",
    "output_data.to_csv('genreForKaggle.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
